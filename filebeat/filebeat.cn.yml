
filebeat.inputs:

- type: log

  # Change to true to enable this input configuration.
  enabled: false

  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /var/log/*.log

  # Exclude lines. A list of regular expressions to match. It drops the lines that are
  # matching any regular expression from the list.
  #exclude_lines: ['^DBG']

  # Include lines. A list of regular expressions to match. It exports the lines that are
  # matching any regular expression from the list.
  #include_lines: ['^ERR', '^WARN']

  # Exclude files. A list of regular expressions to match. Filebeat drops the files that
  # are matching any regular expression from the list. By default, no files are dropped.
  exclude_files: ['trace/.*$']

  # Optional additional fields. These fields can be freely picked
  # to add additional information to the crawled log files for filtering
  fields:
    host: 192.168.1.191
    log_topic: my-replicated-topic

  ### Multiline options
  multiline:
  # Multiline can be used for log messages spanning multiple lines. This is common
  # for Java Stack Traces or C-Line Continuation

      # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [
      pattern: ^\[

      # Defines if the pattern set under pattern should be negated or not. Default is false.
      negate: false

      # Match can be set to "after" or "before". It is used to define if lines should be append to a pattern
      # that was (not) matched before or after or as long as a pattern is not matched based on negate.
      # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash
      match: after

      # The maximum number of lines that are combined to one event.
      # In case there are more the max_lines the additional lines are discarded.
      # Default is 500
      # 合并的最多行数（包含匹配pattern的那一行）
      max_lines: 500

      # After the defined timeout, an multiline event is sent even if no new pattern was found to start a new event
      # Default is 5s.
      #到了timeout之后，即使没有匹配一个新的pattern（发生一个新的事件），也把已经匹配的日志事件发送出去
      timeout: 5s

  # Ignore files which were modified more then the defined timespan in the past.
  # In case all files on your system must be read you can set this value very large.
  # Time strings like 2h (2 hours), 5m (5 minutes) can be used.
  ignore_older: 2h

  # Close older closes the file handler for which were not modified
  # for longer then close_older
  # Time strings like 2h (2 hours), 5m (5 minutes) can be used.
  close_older: 1h


  # Scan frequency in seconds.
  # How often these files should be checked for changes. In case it is set
  # to 0s, it is done as often as possible. Default: 10s
  scan_frequency: 10s

  # Defines the buffer size every harvester uses when fetching the file
  # 每个harvester监控文件时，使用的buffer的大小
  harvester_buffer_size: 16384

  # Maximum number of bytes a single log event can have
  # All bytes after max_bytes are discarded and not sent. The default is 10MB.
  # This is especially useful for multiline log messages which can get large.
  # 日志文件中增加一行算一个日志事件，max_bytes限制在一次日志事件中最多上传的字节数，多出的字节会被丢弃
  max_bytes: 10485760

  # Setting tail_files to true means filebeat starts readding new files at the end
  # instead of the beginning. If this is used in combination with log rotation
  # this can mean that the first entries of a new file are skipped.
  # 如果设置为true，Filebeat从文件尾开始监控文件新增内容，把新增的每一行文件作为一个事件依次发送，
  # 而不是从文件开始处重新发送所有内容
  #tail_files: false

    # Backoff values define how agressively filebeat crawls new files for updates
    # The default values can be used in most cases. Backoff defines how long it is waited
    # to check a file again after EOF is reached. Default is 1s which means the file
    # is checked every second if new lines were added. This leads to a near real time crawling.
    # Every time a new line appears, backoff is reset to the initial value.
    # Filebeat检测到某个文件到了EOF（文件结尾）之后，每次等待多久再去检测文件是否有更新，默认为1s
  backoff: 1s

    # Max backoff defines what the maximum backoff time is. After having backed off multiple times
    # from checking the files, the waiting time will never exceed max_backoff idenependent of the
    # backoff factor. Having it set to 10s means in the worst case a new line can be added to a log
    # file after having backed off multiple times, it takes a maximum of 10s to read the new line
    # Filebeat检测到某个文件到了EOF之后，等待检测文件更新的最大时间，默认是10秒
  max_backoff: 10s

    # The backoff factor defines how fast the algorithm backs off. The bigger the backoff factor,
    # the faster the max_backoff value is reached. If this value is set to 1, no backoff will happen.
    # The backoff value will be multiplied each time with the backoff_factor until max_backoff is reached
    # 定义到达max_backoff的速度，默认因子是2，到达max_backoff后，变成每次等待max_backoff那么长的时间才backoff一次，
    # 直到文件有更新才会重置为backoff
    # 根据现在的默认配置是这样的，每隔1s检测一下文件变化，如果连续检测两次之后文件还没有变化，下一次检测间隔时间变为10s
  backoff_factor: 2


#processors:            #这个地方需要注意，此配置是将日志输出格式过滤掉，一般情况下，一些无用的日志字段我们可以删除，只看关键性的信息
#  - drop_fields:
#      fields: ["beat", "input_type", "source", "offset"]
name: "127.0.0.1"         #设置filebeat收集日志中对应的主机名称，，如果设置为空，这使用该机器的主机名称，这里这是本地IP，便于区分多台主机的日志信息


#============================= Filebeat modules ===============================

#filebeat.config.modules:
  # Glob pattern for configuration loading
#  path: ${path.config}/modules.d/*.yml

  # Set to true to enable config reloading
#  reload.enabled: false

  # Period on which files under path should be checked for changes
  #reload.period: 10s

#==================== Elasticsearch template setting ==========================

#setup.template.settings:
#  index.number_of_shards: 1
  #index.codec: best_compression
  #_source.enabled: false

#================================ General =====================================

# The name of the shipper that publishes the network data. It can be used to group
# all the transactions sent by a single shipper in the web interface.
#name:

# The tags of the shipper are included in their own field with each
# transaction published.
#tags: ["service-X", "web-tier"]

# Optional fields that you can specify to add additional information to the
# output.
#fields:
#  env: staging


#============================== Dashboards =====================================
# These settings control loading the sample dashboards to the Kibana index. Loading
# the dashboards is disabled by default and can be enabled either by setting the
# options here or by using the `setup` command.
#setup.dashboards.enabled: false

# The URL from where to download the dashboards archive. By default this URL
# has a value which is computed based on the Beat name and version. For released
# versions, this URL points to the dashboard archive on the artifacts.elastic.co
# website.
#setup.dashboards.url:

#============================== Kibana =====================================

# Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API.
# This requires a Kibana endpoint configuration.
#setup.kibana:

  # Kibana Host
  # Scheme and port can be left out and will be set to the default (http and 5601)
  # In case you specify and additional path, the scheme is required: http://localhost:5601/path
  # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601
  #host: "localhost:5601"

  # Kibana Space ID
  # ID of the Kibana Space into which the dashboards should be loaded. By default,
  # the Default Space will be used.
  #space.id:

#============================= Elastic Cloud ==================================

# These settings simplify using filebeat with the Elastic Cloud (https://cloud.elastic.co/).

# The cloud.id setting overwrites the `output.elasticsearch.hosts` and
# `setup.kibana.host` options.
# You can find the `cloud.id` in the Elastic Cloud web UI.
#cloud.id:

# The cloud.auth setting overwrites the `output.elasticsearch.username` and
# `output.elasticsearch.password` settings. The format is `<user>:<pass>`.
#cloud.auth:

#================================ Outputs =====================================

# Configure what output to use when sending the data collected by the beat.

#-------------------------- Elasticsearch output ------------------------------
#output.elasticsearch:
#  # Array of hosts to connect to.
#  hosts: ["localhost:9200"]

  # Optional protocol and basic auth credentials.
  #protocol: "https"
  #username: "elastic"
  #password: "changeme"

#----------------------------- Logstash output --------------------------------
#output.logstash:
  # The Logstash hosts
  #hosts: ["localhost:5044"]

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  #ssl.key: "/etc/pki/client/cert.key"

#----------------------------- kafka output --------------------------------
output.kafka:
  enabled: true
  hosts: ["192.168.1.196:9092", "192.168.1.197:9092", "192.168.1.198:9092"]
  topic: '%{[fields][log_topic]}'    #指定要发送数据到kafka集群的哪个topic，与上述的"fields: log_topic："相对应，这是6.x的配置
  partition.round_robin:
    reachable_only: true
  worker: 2
  required_acks: 1
  compression: gzip
  max_message_bytes: 10000000



