
filebeat.inputs:

  - type: log
    enabled: true
    paths:
      - /var/log/*.log

    fields:
      host: 192.168.1.191
      log_topic: my-replicated-topic

    multiline:
      pattern: ^\[
      negate: false
      match: after
      max_lines: 500
      timeout: 5s
    ignore_older: 2h
    close_older: 1h
    scan_frequency: 10s
    harvester_buffer_size: 16384
    max_bytes: 10485760
    backoff: 1s
    max_backoff: 10s
    backoff_factor: 2


#processors:            #这个地方需要注意，此配置是将日志输出格式过滤掉，一般情况下，一些无用的日志字段我们可以删除，只看关键性的信息
#  - drop_fields:
#      fields: ["beat", "input_type", "source", "offset"]
name: "127.0.0.1"         #设置filebeat收集日志中对应的主机名称，，如果设置为空，这使用该机器的主机名称，这里这是本地IP，便于区分多台主机的日志信息

#----------------------------- kafka output --------------------------------
output.kafka:
  enabled: true
  hosts: ["192.168.1.196:9092", "192.168.1.197:9092", "192.168.1.198:9092"]
  topic: '%{[fields][log_topic]}'    #指定要发送数据到kafka集群的哪个topic，与上述的"fields: log_topic："相对应，这是6.x的配置
  partition.round_robin:
    reachable_only: true
  worker: 2
  required_acks: 1
  compression: gzip
  max_message_bytes: 10000000



